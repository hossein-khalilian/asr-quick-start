{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a941bd4-40e2-40bf-9134-ebbed1cfbc2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T07:10:18.810275Z",
     "iopub.status.busy": "2025-07-01T07:10:18.809449Z",
     "iopub.status.idle": "2025-07-01T07:10:18.819664Z",
     "shell.execute_reply": "2025-07-01T07:10:18.817781Z",
     "shell.execute_reply.started": "2025-07-01T07:10:18.810206Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b19858d2-5769-4062-92b3-1fc335227726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51f7041-ac2d-4770-8f76-c6cd63ce026e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T07:10:33.833237Z",
     "iopub.status.busy": "2025-07-01T07:10:33.832457Z",
     "iopub.status.idle": "2025-07-01T07:10:33.841555Z",
     "shell.execute_reply": "2025-07-01T07:10:33.839654Z",
     "shell.execute_reply.started": "2025-07-01T07:10:33.833180Z"
    }
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08675366-b829-4dd5-93c9-1bd90859a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from jiwer import wer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf438a6-bd5c-473f-abc6-35ed7768a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: to save or play audio\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36287853-d3e9-4220-ba71-f061a1ef58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lang_code = \"fa_ir\"\n",
    "num_samples = 20  # use more for full evaluation\n",
    "save_audio = False  # set to True to save files\n",
    "play_audio = False  # set to True in notebooks to play files\n",
    "\n",
    "models = [\n",
    "    \"openai/whisper-small\",\n",
    "    \"m3hrdadfi/wav2vec2-large-xlsr-persian\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "585e1a11-bf84-471f-a6f3-f256338e48ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# --- Load Persian FLEURS dataset ---\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"google/fleurs\", lang_code, split=f\"test[:{num_samples}]\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c659bac5-af5a-41f6-b20f-51ffd5884f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Evaluating model: openai/whisper-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:36<00:00,  1.83s/it]\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WER for openai/whisper-small on FLEURS (Persian): 0.547\n",
      "\n",
      "üîç Evaluating model: m3hrdadfi/wav2vec2-large-xlsr-persian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 630.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on sample 0: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 1: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 2: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 3: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 4: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 5: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 6: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 7: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 8: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 9: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 10: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 11: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 12: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 13: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 14: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 15: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 16: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 17: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 18: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "Error on sample 19: CTC can either predict character level timestamps, or word level timestamps. Set `return_timestamps='char'` or `return_timestamps='word'` as required.\n",
      "‚úÖ WER for m3hrdadfi/wav2vec2-large-xlsr-persian on FLEURS (Persian): 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation loop ---\n",
    "for model_name in models:\n",
    "    print(f\"\\nüîç Evaluating model: {model_name}\")\n",
    "    asr = pipeline(\"automatic-speech-recognition\", model=model_name, device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "    references, hypotheses = [], []\n",
    "\n",
    "    for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        audio_array = sample['audio']['array']\n",
    "        sampling_rate = sample['audio']['sampling_rate']\n",
    "        reference_text = sample['transcription'].strip().lower()\n",
    "\n",
    "        try:\n",
    "            # --- Run ASR ---\n",
    "            result = asr(audio_array, chunk_length_s=30, return_timestamps=False)\n",
    "            predicted_text = result[\"text\"].strip().lower()\n",
    "\n",
    "            # --- Collect results ---\n",
    "            references.append(reference_text)\n",
    "            hypotheses.append(predicted_text)\n",
    "\n",
    "            # --- Optional: Save or play audio ---\n",
    "            if save_audio:\n",
    "                out_path = f\"audio_{idx}.wav\"\n",
    "                sf.write(out_path, audio_array, sampling_rate)\n",
    "            if play_audio:\n",
    "                display(Audio(data=audio_array, rate=sampling_rate))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on sample {idx}: {e}\")\n",
    "\n",
    "    # --- Calculate and display WER ---\n",
    "    error = wer(references, hypotheses)\n",
    "    print(f\"‚úÖ WER for {model_name} on FLEURS (Persian): {error:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c81591-a881-4172-86ff-6e5fdebffae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
